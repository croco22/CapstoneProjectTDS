{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/croco22/CapstoneProjectTDS/blob/philipp/notebooks/05_Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Task 3: Dashboard\n",
        "An interactive dashboard built as a Gradio app for testing the answering of questionnaires in a user-friendly way. Responses are processed in the background using a Huggingface model to provide relevant insights and evaluations. Additionally, a speech-to-text model has been integrated, allowing users to answer questions verbally, which are then automatically transcribed into text.\n",
        "\n",
        "**Note**: The secrets `GOOGLE_API_KEY` and `HF_TOKEN` must be configured in your Colab environment for proper execution."
      ],
      "metadata": {
        "id": "a_5wicdWgEvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup"
      ],
      "metadata": {
        "id": "m5YYj8uzulcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install qrcode[pil]\n",
        "!pip install SpeechRecognition\n",
        "!pip install gradio\n",
        "!pip install dateparser\n",
        "# !pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import dateparser\n",
        "import pytz\n",
        "import qrcode\n",
        "import requests\n",
        "import speech_recognition as sr\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from IPython.display import display, Image\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from transformers import pipeline\n",
        "# import whisper\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "n3aIGKcngDpm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recognizer = sr.Recognizer()\n",
        "timezone = pytz.timezone('Europe/Berlin')\n",
        "\n",
        "# API Setup\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "userdata.get('HF_TOKEN') # optional\n",
        "\n",
        "# Download favicon\n",
        "image_url = \"https://raw.githubusercontent.com/croco22/CapstoneProjectTDS/refs/heads/main/images/profile.jpg\"\n",
        "img_data = requests.get(image_url).content\n",
        "with open('favicon.png', 'wb') as handler:\n",
        "    handler.write(img_data)\n",
        "\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Generates text based on the provided prompt using the genai model. The function sends the prompt\n",
        "    to the model, with a generation configuration that includes a temperature of 2.0 for creative output.\n",
        "    It then waits for 5 seconds to avoid exceeding API limits before returning the generated text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.GenerationConfig(\n",
        "                temperature=2.0, # creative output\n",
        "            )\n",
        "        )\n",
        "        time.sleep(5) # avoid exceeding API limits\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        exit(\"Error during API call: \", e)"
      ],
      "metadata": {
        "id": "LEld6V8nspjK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data from the provided questionnaires"
      ],
      "metadata": {
        "id": "l6lNDb8ERWJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = list()\n",
        "\n",
        "for q in range(1, 6):\n",
        "    url = f'https://raw.githubusercontent.com/croco22/CapstoneProjectTDS/refs/heads/main/questionnaires/questionnaire{q}.json'\n",
        "    temp_df = pd.read_json(url)\n",
        "\n",
        "    # Unpack options into an array\n",
        "    temp_df['options'] = temp_df['options'].apply(lambda x: [option['option'] for option in x])\n",
        "\n",
        "    # Remove options for specific question types\n",
        "    # because irrelevant or do not contribute meaningfully to the dataset\n",
        "    temp_df.loc[temp_df['type'].isin(['TEXT', 'NUMBER', 'DATE']), 'options'] = None\n",
        "\n",
        "    # Include the name of the questionnaire\n",
        "    temp_df['questionnaire'] = f\"Questionnaire {q}\"\n",
        "\n",
        "    dfs.append(temp_df)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "FT5Y15cZQijj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rephrase Questions\n",
        "The reason for rephrasing the questions is that some questions in the questionnaire may consist of only single words or are not structured as proper questions. By rephrasing them, we ensure that the questions are clear, complete, and easy for the user to understand.\n",
        "\n",
        "**Warning**: This cell may take a few minutes to execute as it calls the Gemini API a few times."
      ],
      "metadata": {
        "id": "7A2T2D-_Ru2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rephrase_question(text):\n",
        "    prompt = f\"\"\"\n",
        "        Reformulate the following statement into a clear, concise, and\n",
        "        grammatically correct question that directly addresses the user. For\n",
        "        example, 'Do you consent to data processing?' or 'What kind of follow-up\n",
        "        would you prefer?'. If the text is already a question, preserve its\n",
        "        meaning and intent without altering the content or facts. The statement\n",
        "        is: '{text}'. Return only the reformulated question, without any\n",
        "        additional explanations, comments, or text.\n",
        "    \"\"\"\n",
        "    return generate_text(prompt)\n",
        "\n",
        "\n",
        "df['rephrased_question'] = df['question'].apply(rephrase_question)\n",
        "\n",
        "df = df[['questionnaire', 'type', 'rephrased_question', 'options']].copy()\n",
        "df['answer'] = \"<skipped by user>\"\n",
        "df['missed_count'] = 0\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PRffJkreal_J",
        "outputId": "b2d3c976-bfa0-467e-9de7-3620a4de2558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     questionnaire           type                     rephrased_question  \\\n",
              "0  Questionnaire 1  SINGLE_SELECT     Do you consent to data processing?   \n",
              "1  Questionnaire 1  SINGLE_SELECT                  Which customer group?   \n",
              "2  Questionnaire 1   MULTI_SELECT  Which products are you interested in?   \n",
              "3  Questionnaire 1   MULTI_SELECT     What kind of follow-up is planned?   \n",
              "4  Questionnaire 1   MULTI_SELECT    Who should I copy on the follow-up?   \n",
              "\n",
              "                                             options             answer  \\\n",
              "0                                          [Yes, No]  <skipped by user>   \n",
              "1  [End User, Wholesaler, Distributor, Consultant...  <skipped by user>   \n",
              "2  [MY-SYSTEM, Notion, JTS, JS EcoLine, AKW100, A...  <skipped by user>   \n",
              "3        [Email, Phone, Schedule a Visit, No action]  <skipped by user>   \n",
              "4  [Stephan Maier, Joachim Wagner, Erik Schneider...  <skipped by user>   \n",
              "\n",
              "   missed_count  \n",
              "0             0  \n",
              "1             0  \n",
              "2             0  \n",
              "3             0  \n",
              "4             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80c2b9bb-e1d0-4ace-81be-92993cc16858\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionnaire</th>\n",
              "      <th>type</th>\n",
              "      <th>rephrased_question</th>\n",
              "      <th>options</th>\n",
              "      <th>answer</th>\n",
              "      <th>missed_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Questionnaire 1</td>\n",
              "      <td>SINGLE_SELECT</td>\n",
              "      <td>Do you consent to data processing?</td>\n",
              "      <td>[Yes, No]</td>\n",
              "      <td>&lt;skipped by user&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Questionnaire 1</td>\n",
              "      <td>SINGLE_SELECT</td>\n",
              "      <td>Which customer group?</td>\n",
              "      <td>[End User, Wholesaler, Distributor, Consultant...</td>\n",
              "      <td>&lt;skipped by user&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Questionnaire 1</td>\n",
              "      <td>MULTI_SELECT</td>\n",
              "      <td>Which products are you interested in?</td>\n",
              "      <td>[MY-SYSTEM, Notion, JTS, JS EcoLine, AKW100, A...</td>\n",
              "      <td>&lt;skipped by user&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Questionnaire 1</td>\n",
              "      <td>MULTI_SELECT</td>\n",
              "      <td>What kind of follow-up is planned?</td>\n",
              "      <td>[Email, Phone, Schedule a Visit, No action]</td>\n",
              "      <td>&lt;skipped by user&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Questionnaire 1</td>\n",
              "      <td>MULTI_SELECT</td>\n",
              "      <td>Who should I copy on the follow-up?</td>\n",
              "      <td>[Stephan Maier, Joachim Wagner, Erik Schneider...</td>\n",
              "      <td>&lt;skipped by user&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80c2b9bb-e1d0-4ace-81be-92993cc16858')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80c2b9bb-e1d0-4ace-81be-92993cc16858 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80c2b9bb-e1d0-4ace-81be-92993cc16858');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bc85f102-c6f9-4414-82d8-dace0042cd92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc85f102-c6f9-4414-82d8-dace0042cd92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bc85f102-c6f9-4414-82d8-dace0042cd92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"questionnaire\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Questionnaire 2\",\n          \"Questionnaire 5\",\n          \"Questionnaire 3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"MULTI_SELECT\",\n          \"NUMBER\",\n          \"TEXT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rephrased_question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Do you have any notes?\",\n          \"What's your phone number?\",\n          \"Do you consent to data processing?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"options\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"<skipped by user>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missed_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Context\n",
        "We explored an alternative approach, which unexpectedly led to very promising results. This new method involves evaluating the context using different models and categorizing questions into different types. By understanding the nature of each question, we can apply more targeted processing techniques that improve the overall accuracy and relevance of the answers. This approach has shown to outperform the initial Q&A models, providing more robust and contextually aware results."
      ],
      "metadata": {
        "id": "_i48UdQONz7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity (deprecated)\n",
        "The code evaluates the relevance of questions and answers based on their similarity to a given context or message. It computes scores for each question or answer by combining the results from a Q&A model with cosine similarity between embeddings. The most relevant question or answer is selected based on the highest combined score, ensuring that the response aligns closely with the provided context or query. This approach leverages both semantic understanding and contextual relevance to improve the selection process.\n",
        "\n",
        "After extensive testing, we decided to use **Zero-Shot Classification** in our final submission because it delivered the best results."
      ],
      "metadata": {
        "id": "5DQgleo9_Adj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_number(text):\n",
        "#     prompt = f\"\"\"\n",
        "#         Following text contains a phone number: '{text}'.\n",
        "#         Extract the phone number and only return the phone number, without any\n",
        "#         additional explanations, comments, or text.\n",
        "#     \"\"\"\n",
        "#     return generate_text(prompt)\n",
        "\n",
        "# def get_date(text):\n",
        "#     prompt = f\"\"\"\n",
        "#         Following text contains a date/time reference: '{text}'.\n",
        "#         Extract the time reference and return an integer value of this reference in seconds.\n",
        "#         E.g. 'in three weeks' should return '1814400', 'tomorrow' should return '86400'.\n",
        "#         Return the value without any additional explanations, comments, or text.\n",
        "#     \"\"\"\n",
        "#     result = generate_text(prompt)\n",
        "#     try:\n",
        "#         int_reference = int(result)\n",
        "#     except ValueError:\n",
        "#         int_reference = 0\n",
        "#     current_time = datetime.now(timezone)\n",
        "#     future_time = current_time + pd.Timedelta(seconds=int_reference)\n",
        "#     formatted_time = future_time.strftime(\"%d.%m.%Y\")\n",
        "#     return formatted_time"
      ],
      "metadata": {
        "id": "3kJgCX3yyzpk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# def evaluate_context(context, unanswered_questions):\n",
        "#     best_question = None\n",
        "#     highest_score = -1\n",
        "#     context_embedding = embedding_model.encode([context])\n",
        "\n",
        "#     for question in unanswered_questions:\n",
        "#         qa_result = qa_pipeline(question=question, context=context)\n",
        "#         score = qa_result['score']\n",
        "\n",
        "#         question_embedding = embedding_model.encode([question])\n",
        "#         similarity = cosine_similarity(context_embedding, question_embedding)[0][0]\n",
        "\n",
        "#         combined_score = score * similarity\n",
        "\n",
        "#         if combined_score > highest_score:\n",
        "#             highest_score = combined_score\n",
        "#             best_question = question\n",
        "\n",
        "#     return best_question\n",
        "\n",
        "# def evaluate_answer(message, options):\n",
        "#     message_embedding = embedding_model.encode([message])\n",
        "\n",
        "#     highest_similarity = -1\n",
        "#     best_answer = None\n",
        "\n",
        "#     for option in options:\n",
        "#         option_embedding = embedding_model.encode([option])\n",
        "#         similarity = cosine_similarity([message_embedding[0]], [option_embedding[0]])[0][0]\n",
        "\n",
        "#         if similarity > highest_similarity:\n",
        "#             highest_similarity = similarity\n",
        "#             best_answer = option\n",
        "\n",
        "#     return best_answer"
      ],
      "metadata": {
        "id": "DETYkkFVdlvL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Classification\n",
        "The code evaluates a given context against a list of potential questions or answer types based on the specified question type. It uses a zero-shot classification model (such as BART or DeBERTa) to match the context to the most relevant question:\n",
        "\n",
        "- For **single select questions** (`SINGLE_SELECT`), it identifies the most relevant question based on the highest classification score.\n",
        "- For **multi-select questions** (`MULTI_SELECT`), it returns all questions with a classification score above a threshold (0.25).\n",
        "- For **text-based questions** (`TEXT`), it simply returns the context itself.\n",
        "- For **numeric questions** (`NUMBER`), it processes the context to extract a number.\n",
        "- For **date-related questions** (`DATE`), it processes the context to extract a date.\n",
        "\n",
        "If none of these categories match, it returns an error message indicating that evaluation is not possible."
      ],
      "metadata": {
        "id": "Pq38WxBc_qzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "def evaluate_context(context, questions, qtype=None, real_question=None):\n",
        "    if qtype is None or qtype == \"SINGLE_SELECT\":\n",
        "        question_results = classifier(context, candidate_labels=questions)\n",
        "        best_match = question_results[\"labels\"][0]\n",
        "        return best_match\n",
        "    elif qtype == \"MULTI_SELECT\":\n",
        "        question_results = classifier(context, candidate_labels=questions, multi_label=True)\n",
        "        best_matches = [\n",
        "            label for label, score in zip(question_results[\"labels\"], question_results[\"scores\"])\n",
        "            if score > 0.25 # threshold was determined empirically after testing different values\n",
        "        ]\n",
        "        # Ensure at least one answer is selected\n",
        "        if not best_matches:\n",
        "            best_matches = [question_results[\"labels\"][0]]\n",
        "        return \", \".join(map(str, best_matches))\n",
        "    elif qtype == \"TEXT\":\n",
        "        return context\n",
        "    elif qtype == \"NUMBER\":\n",
        "        result = qa_pipeline(question=real_question, context=context)\n",
        "        return result['answer']\n",
        "    elif qtype == \"DATE\":\n",
        "        current_time = datetime.now(timezone)\n",
        "        result = qa_pipeline(question=real_question, context=context)\n",
        "        time_string = \"in \" + result['answer'] # make sure the reference is in the future\n",
        "        parsed_date = dateparser.parse(time_string, settings={\"RELATIVE_BASE\": current_time})\n",
        "        if parsed_date is None or parsed_date < current_time:\n",
        "            return current_time.strftime(\"%d.%m.%Y\")\n",
        "        else: # parsing successful\n",
        "            return parsed_date.strftime(\"%d.%m.%Y\")\n",
        "    else:\n",
        "        return \"[ERROR] Evaluation not possible.\""
      ],
      "metadata": {
        "id": "J_FGXapaUI0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256ae57e-ef64-4d34-ad94-03f718916690"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1113, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "ValueError: I/O operation on closed file\n",
            "Call stack:\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-105-a23d3e36b683>\", line 1, in <cell line: 0>\n",
            "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\", line 1178, in pipeline\n",
            "    return pipeline_class(model=model, framework=framework, task=task, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/zero_shot_classification.py\", line 89, in __init__\n",
            "    super().__init__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 916, in __init__\n",
            "    logger.warning(f\"Device set to use {self.device}\")\n",
            "Message: 'Device set to use cpu'\n",
            "Arguments: ()\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1113, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "ValueError: I/O operation on closed file\n",
            "Call stack:\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-105-a23d3e36b683>\", line 2, in <cell line: 0>\n",
            "    qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\", line 1178, in pipeline\n",
            "    return pipeline_class(model=model, framework=framework, task=task, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py\", line 270, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 916, in __init__\n",
            "    logger.warning(f\"Device set to use {self.device}\")\n",
            "Message: 'Device set to use cpu'\n",
            "Arguments: ()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Application\n",
        "In this section, the backend methods for the Gradio application are defined, including the logic for processing user inputs and generating responses. The necessary functionality, such as handling questionnaire data and integrating models for text and speech recognition, is implemented. Additionally, CSS styles are applied to customize the visual design of the interface, ensuring that the user experience is both intuitive and visually appealing."
      ],
      "metadata": {
        "id": "YJ8BIxwGR9Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    padding-left: 15% !important;\n",
        "    padding-right: 15% !important;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0zWUBoCRRAj4"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_message(message, unanswered_questions, df_v):\n",
        "    \"\"\"\n",
        "    Evaluates a given message by finding the most relevant question from the unanswered questions list.\n",
        "    It uses the `evaluate_context` function to match the message with the appropriate question and then\n",
        "    determines the possible answer options and question type from a DataFrame. The answer is then evaluated\n",
        "    based on the message and stored in the DataFrame. The function also tracks the number of missed questions\n",
        "    and updates the unanswered questions list accordingly.\n",
        "    \"\"\"\n",
        "    # Get value of state object\n",
        "    if isinstance(unanswered_questions, gr.State):\n",
        "        uq_list = unanswered_questions.value\n",
        "    else:\n",
        "        uq_list = unanswered_questions\n",
        "\n",
        "    real_question = evaluate_context(message, uq_list)\n",
        "    qtype = df_v[df_v['rephrased_question'] == real_question]['type'].iloc[0]\n",
        "    answer_options = df_v[df_v['rephrased_question'] == real_question]['options'].iloc[0]\n",
        "    df_v.loc[df_v['rephrased_question'] == real_question, 'answer'] = evaluate_context(message, answer_options, qtype, real_question)\n",
        "\n",
        "    # Increment missed_count for each skipped question\n",
        "    real_idx = uq_list.index(real_question)\n",
        "    df_v.loc[(df_v['rephrased_question'].isin(uq_list[:real_idx])), 'missed_count'] += 1\n",
        "\n",
        "    # Remove the answered question\n",
        "    uq_list.remove(real_question)\n",
        "\n",
        "    # Remove the intentionally skipped questions\n",
        "    remaining_uq_list = [question for question in uq_list if df_v[df_v['rephrased_question'] == question]['missed_count'].iloc[0] <= 1]\n",
        "\n",
        "    return remaining_uq_list, df_v"
      ],
      "metadata": {
        "id": "w-YiHZ1gNgLh"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_response(unanswered_questions, df_v=None):\n",
        "    \"\"\"\n",
        "    Generates a response message listing the unanswered questions, which can be displayed in a user interface.\n",
        "    If a DataFrame is provided, it highlights questions that have been skipped once and shows them in a special color.\n",
        "    If no unanswered questions are left, the function returns a thank-you message.\n",
        "    \"\"\"\n",
        "    # Get value of state object\n",
        "    if isinstance(unanswered_questions, gr.State):\n",
        "        uq_list = unanswered_questions.value\n",
        "    else:\n",
        "        uq_list = unanswered_questions\n",
        "\n",
        "    # Return thank-you message if no questions left\n",
        "    if len(uq_list) == 0:\n",
        "            return \"Thank you! Don't forget to hit <i>Send</i> to submit your responses! ðŸ¤—\"\n",
        "\n",
        "    flag = False # answer contains any marked questions\n",
        "\n",
        "    if df_v is None: # only called by start_chat()\n",
        "        unanswered_questions_string = '<ol style=\"font-size: 13px;\">' + ''.join([f'<li>{q}</li>' for q in uq_list]) + '</ol>'\n",
        "    else:\n",
        "        unanswered_questions_string = '<ol style=\"font-size: 13px;\">'\n",
        "        for q in uq_list:\n",
        "            missed_count_row = df_v[df_v['rephrased_question'] == q]\n",
        "            missed_count = missed_count_row['missed_count'].iloc[0]\n",
        "            if missed_count == 1:\n",
        "                flag = True\n",
        "                unanswered_questions_string += f'<li style=\"color: #77f7d1;\">{q}</li>'\n",
        "            else:\n",
        "                unanswered_questions_string += f'<li>{q}</li>'\n",
        "        unanswered_questions_string += '</ol>'\n",
        "\n",
        "    if flag:\n",
        "        response = f\"\"\"\n",
        "            <p>Please answer the following questions<span style='color: #77f7d1;'>*</span>:</p>\n",
        "            <p>{unanswered_questions_string}</p>\n",
        "            <p style='color: #77f7d1; font-size: 10px;'>* Highlighted questions will be skipped if unanswered</p>\n",
        "        \"\"\"\n",
        "    else:\n",
        "        response = f\"\"\"\n",
        "            <p>Please answer the following questions:</p>\n",
        "            <p>{unanswered_questions_string}</p>\n",
        "        \"\"\"\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "paqufcz1DZNj"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions called by gradio instance below\n",
        "\n",
        "def start_chat(selected_questionnaire, history):\n",
        "    q = df[df['questionnaire'] == selected_questionnaire]['rephrased_question'].tolist()\n",
        "    history.append({\"role\": \"assistant\", \"content\": set_response(q)})\n",
        "    return (\n",
        "        gr.update(visible=False),\n",
        "        gr.update(visible=False),\n",
        "        gr.update(value=history, visible=True),\n",
        "        gr.update(visible=True),\n",
        "        gr.update(visible=True),\n",
        "        gr.State(q)\n",
        "    )\n",
        "\n",
        "\n",
        "def add_message(history, message, unanswered_questions, df_state):\n",
        "    # Get value of state object\n",
        "    if isinstance(df_state, gr.State):\n",
        "        df_v = df_state.value\n",
        "    else:\n",
        "        df_v = df_state\n",
        "\n",
        "    if len(message[\"files\"]) > 0: # voice input\n",
        "        # model_turbo = whisper.load_model(\"turbo\")\n",
        "        # result = model_turbo.transcribe(message[\"files\"][0])\n",
        "        # message = result[\"text\"]\n",
        "        with sr.AudioFile(message[\"files\"][0]) as source:\n",
        "            audio_data = recognizer.record(source)\n",
        "        try:\n",
        "            message = recognizer.recognize_google(audio_data)\n",
        "        except sr.UnknownValueError:\n",
        "            exit(\"Audio could not be understood\")\n",
        "        except sr.RequestError as e:\n",
        "            exit(f\"Error with the request: {e}\")\n",
        "    elif message[\"text\"] is not None and message[\"text\"] != \"\":\n",
        "        message = message[\"text\"]\n",
        "    else:\n",
        "        message = \"<auto> I'm a retailer.\"\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    uq_list, df_new = evaluate_message(message, unanswered_questions, df_v)\n",
        "    if len(uq_list) == 0:\n",
        "        return history, gr.update(visible=False), gr.State([]), gr.State(df_new)\n",
        "    else:\n",
        "        return history, gr.MultimodalTextbox(value=None), gr.State(uq_list), gr.State(df_new)\n",
        "\n",
        "\n",
        "def bot(history, unanswered_questions, df_state):\n",
        "    # Get value of state object\n",
        "    if isinstance(df_state, gr.State):\n",
        "        df_v = df_state.value\n",
        "    else:\n",
        "        df_v = df_state\n",
        "\n",
        "    response = set_response(unanswered_questions, df_v)\n",
        "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "    for character in response: # typing effect\n",
        "        history[-1][\"content\"] += character\n",
        "        time.sleep(0.006)\n",
        "        yield history\n",
        "\n",
        "\n",
        "def download(selected_questionnaire, df_state):\n",
        "    # Get value of state object\n",
        "    if isinstance(df_state, gr.State):\n",
        "        df_v = df_state.value\n",
        "    else:\n",
        "        df_v = df_state\n",
        "\n",
        "    current_timestamp = datetime.now(timezone)\n",
        "    formatted_timestamp = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"answers_{formatted_timestamp}.json\"\n",
        "\n",
        "    filtered_df = df_v[df_v['questionnaire'] == selected_questionnaire]\n",
        "    filtered_df = filtered_df[['rephrased_question', 'answer']].rename(columns={\"rephrased_question\": \"question\"})\n",
        "    filtered_df.to_json(filename, orient='records', indent=4)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def reset_state(original_df_state):\n",
        "    return (\n",
        "        gr.update(visible=True),\n",
        "        gr.update(visible=True),\n",
        "        gr.update(value=list(), visible=False),\n",
        "        gr.update(value=dict(), visible=False),\n",
        "        gr.update(visible=False),\n",
        "        gr.State([]),\n",
        "        original_df_state\n",
        "    )"
      ],
      "metadata": {
        "id": "h5L88rxgNgf7"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme='Nymbo/Nymbo_Theme', title=\"Philipp's Chatbot\", css=custom_css) as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "        # Questionnaire Chatbot ðŸ‘¨â€ðŸ’»ðŸš€\n",
        "        ### by Philipp Landeck\n",
        "        <br>\n",
        "    \"\"\")\n",
        "\n",
        "    unanswered_questions = gr.State([])\n",
        "    df_state = gr.State(df)\n",
        "    original_df_state = gr.State(df)\n",
        "\n",
        "    dropdown = gr.Dropdown(\n",
        "        choices=list(df['questionnaire'].unique()),\n",
        "        label=\"Select a questionnaire\",\n",
        "        interactive=True,\n",
        "        visible=True,\n",
        "        value=df['questionnaire'].iloc[0]\n",
        "    )\n",
        "\n",
        "    start_button = gr.Button(\"Start\", visible=True)\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        show_label=False,\n",
        "        type=\"messages\",\n",
        "        visible=False,\n",
        "        height=500\n",
        "    )\n",
        "\n",
        "    chat_input = gr.MultimodalTextbox(\n",
        "        interactive=True,\n",
        "        sources=['microphone'],\n",
        "        placeholder=\"Enter message or record voice...\",\n",
        "        show_label=False,\n",
        "        autofocus=True,\n",
        "        visible=False\n",
        "    )\n",
        "\n",
        "    # Call download() function\n",
        "    send_button = gr.DownloadButton(\"Send\", value=download, visible=False, inputs=[dropdown, df_state])\n",
        "\n",
        "    # Call start_chat() function\n",
        "    start_button.click(\n",
        "        start_chat, [dropdown, chatbot], [dropdown, start_button, chatbot, chat_input, send_button, unanswered_questions]\n",
        "    )\n",
        "\n",
        "    # Call add_message() function\n",
        "    chat_msg = chat_input.submit(\n",
        "        add_message, [chatbot, chat_input, unanswered_questions, df_state], [chatbot, chat_input, unanswered_questions, df_state]\n",
        "    )\n",
        "\n",
        "    # Call bot() function\n",
        "    chat_msg.then(bot, [chatbot, unanswered_questions, df_state], chatbot)\n",
        "\n",
        "    # Call reset_state() function\n",
        "    send_button.click(\n",
        "        reset_state, [original_df_state], [dropdown, start_button, chatbot, chat_input, send_button, unanswered_questions, df_state]\n",
        "    )"
      ],
      "metadata": {
        "id": "Pb24QzeVADPb"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qr_code(url):\n",
        "    qr = qrcode.QRCode(version=1, box_size=8, border=2)\n",
        "    qr.add_data(url)\n",
        "    qr.make(fit=True)\n",
        "    img = qr.make_image(fill='black', back_color='white')\n",
        "    img_path = 'gradio_code.png'\n",
        "    img.save(img_path)\n",
        "    return img_path"
      ],
      "metadata": {
        "id": "-NKFlacWEqSh"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Gradio App\n",
        "This code captures the public URL of a Gradio app launched with `app.launch()`. It redirects the output, extracts the Gradio URL using a regex, and generates a QR code for the URL. If no URL is found, the program exits with an error message.\n",
        "\n",
        "**Warning**: This cell redirects stdout. If execution is interrupted, manually reset stdout to `original_stdout` or reset the notebook's global variables to restore normal printing functionality."
      ],
      "metadata": {
        "id": "xSMbhtPCErYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround to get the public URL from stdout\n",
        "original_stdout = sys.stdout\n",
        "\n",
        "output = io.StringIO()\n",
        "sys.stdout = output # set print to output variable\n",
        "\n",
        "app.launch(share=True, show_api=False, inline=False, favicon_path='/content/favicon.png')\n",
        "\n",
        "sys.stdout = original_stdout # reset print to stdout\n",
        "\n",
        "captured_output = output.getvalue()\n",
        "\n",
        "link_pattern = r'(https://.*\\.gradio\\.live)'\n",
        "re_match = re.search(link_pattern, captured_output)\n",
        "\n",
        "if re_match:\n",
        "    public_url = re_match.group(1)\n",
        "    print(f\"Public URL (click to open the app): {public_url}\\n\")\n",
        "    qr_image_path = generate_qr_code(public_url)\n",
        "    display(Image(qr_image_path))\n",
        "else:\n",
        "    exit(\"No URL found in the captured output.\")"
      ],
      "metadata": {
        "id": "4BJIh5WwIoq_",
        "outputId": "eb26f0e0-aaf7-42e4-ddba-55d182a6ef1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL (click to open the app): https://24e3f9f4b336168477.gradio.live\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAEIAQAAAACLjVdSAAABhUlEQVR4nO1ZQRLCQAhLnP7/y/EQYLdevFHGtdrtuM0hZVIISOHL8foG+CN+GAGlQATFAvni21OY9iAuwM9OCBAFUcTaHcO0Lx6gn56gvHLbncK0HUEIIIX1tjzCYxiCQ3g8g7gApBxEMdKG1u4Upn3x2BUhxqd2pzDtQXClCkLxvXm0KUzb9EEBVWOjstRPjWHa6McESXZhtmPn+rEXIFEERbsPpDeDdJ7/SH1AgqwSSyPEcpo+7NdFMOvMqrUCNIfpI/VFZd0rpU5h2oYgCDIioIhGtrujmDYgXGxX0Q0nJguHJ+oDYTgchj2LnJk/GIY0excgZyA6Tx9X5U+/IyyTCtm1TmHaWV+IioQYTV3uT2Ha6MfWELV8WN47zY/d/Ed6DssjzilMO/UBbKN1X3M9TB+f8/U1BUkTMoVpXzxikk67jwxLvktTmDbGw4c8GQtpgPF/zBSm7fFw1a2+JfvdKUwb47FSagrEEyKwjccUxDZfr5ZlG6Uel0/vs/TnePwRMxFvbHLVxnjevHoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}